{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwiks9635/Mini-Agentic-AI/blob/main/Agentic_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AI Engineer Assignment**"
      ],
      "metadata": {
        "id": "Ez24WtedPxIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langgraph\n",
        "!pip install pinecone-client[grpc]\n",
        "!pip install -U langsmith\n",
        "!pip install pypdf\n",
        "!pip install streamlit\n",
        "!pip install -U langchain-community\n",
        "!pip install -qU langchain-pinecone"
      ],
      "metadata": {
        "id": "8mMKR6FYy63u",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U -q \"google-generativeai>=0.8.3\"\n",
        "%pip install --upgrade --quiet  langchain-google-genai"
      ],
      "metadata": {
        "id": "PFezMtjay68_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "import langchain\n",
        "import langgraph\n",
        "from langgraph.graph import Graph\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "from langsmith import traceable, Client\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from typing import TypedDict, Optional\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.schema import Document"
      ],
      "metadata": {
        "id": "KuAK8ROZy7DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import traceable, Client"
      ],
      "metadata": {
        "id": "PwPok2tfuNrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup Pinecone**"
      ],
      "metadata": {
        "id": "k57HqswBFZDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Pinecone index\n",
        "index_name = 'qa-bot'\n",
        "PINE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "from pinecone.grpc import PineconeGRPC as Pinecone\n",
        "from pinecone import ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=PINE_API_KEY)\n",
        "\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=768,\n",
        "    metric='cosine',\n",
        "    spec=ServerlessSpec(\n",
        "      cloud=\"aws\",\n",
        "      region=\"us-east-1\"\n",
        "    )\n",
        "  )\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "Id8-JhgMy7Kd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Build LangGraph Node**"
      ],
      "metadata": {
        "id": "_ghVyMR5ueBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PipelineState(TypedDict):\n",
        "    user_query: str\n",
        "    action: Optional[str]\n",
        "    response: Optional[str]"
      ],
      "metadata": {
        "id": "jbfp8yauXqrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_node(state: PipelineState):\n",
        "    query = state[\"user_query\"]\n",
        "\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
        "\n",
        "    classification_prompt = f\"\"\"\n",
        "    Classify this query as either 'weather', 'rag', or 'general'.\n",
        "    - If it is asking about weather conditions, classify as 'weather'.\n",
        "    - If it is asking about information from a provided document, classify as 'rag'.\n",
        "    - If the query does not fit into these categories, classify as 'general'.\n",
        "\n",
        "    Query: {query}\n",
        "    \"\"\"\n",
        "\n",
        "    action = llm.invoke(classification_prompt)\n",
        "\n",
        "    # Ensure action is valid\n",
        "    action_str = action.content.lower()\n",
        "    if action_str not in [\"weather\", \"rag\", \"general\"]:\n",
        "        action_str = \"general\"\n",
        "    else :\n",
        "        action_str = action.content.lower()\n",
        "\n",
        "    return {\"action\": action_str}"
      ],
      "metadata": {
        "id": "yWdmqgihUPO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Weather API Call**"
      ],
      "metadata": {
        "id": "WYcSgKntwFzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0)\n",
        "\n",
        "# Prompt template for extracting city name\n",
        "city_extraction_prompt = PromptTemplate.from_template(\n",
        "    \"Extract the city name from this query: {query}. If no city is found, return 'None'.\"\n",
        ")\n",
        "\n",
        "def fetch_weather(state: PipelineState):\n",
        "    user_query = state[\"user_query\"]\n",
        "\n",
        "    # Ask Gemini LLM to extract the city name\n",
        "    prompt = city_extraction_prompt.format(query=user_query)\n",
        "    city = llm.invoke(prompt)\n",
        "\n",
        "    if city.content.lower() == \"none\":\n",
        "        return {\"response\": \"I couldn't determine the city name. Please specify a valid location.\"}\n",
        "\n",
        "    api_key = os.getenv(\"OPENWEATHER_API_KEY\")\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city.content}&appid={api_key}&units=metric\"\n",
        "\n",
        "    response = requests.get(url).json()\n",
        "\n",
        "    if response.get(\"main\"):\n",
        "        weather_info = f\"Current weather in {city.content}: {response['weather'][0]['description']}, Temperature: {response['main']['temp']}Â°C.\"\n",
        "    else:\n",
        "        weather_info = \"Weather data not available. Please check the city name.\"\n",
        "\n",
        "    return {\"response\": weather_info}"
      ],
      "metadata": {
        "id": "Kt8799r5cAhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PDF\n",
        "def load_and_split_pdf(pdf_path):\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into smaller chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    return docs"
      ],
      "metadata": {
        "id": "02Qa3LX6U2Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  **Generate Embeddings**"
      ],
      "metadata": {
        "id": "Aeba6ZdGx9Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def get_gemini_embeddings(texts, batch_size = 10):\n",
        "    model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", task_type=\"retrieval_document\")\n",
        "    embeddings = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i : i + batch_size]\n",
        "\n",
        "        try:\n",
        "            response = model.embed_documents(batch)\n",
        "            batch_embeddings = [item for item in response]\n",
        "            embeddings.extend(batch_embeddings)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {i}: {e}\")\n",
        "            time.sleep(5)\n",
        "    return np.array(embeddings)"
      ],
      "metadata": {
        "id": "xAPp2FRvV72d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Strore Embedding in Pinecone**"
      ],
      "metadata": {
        "id": "TVDaXloCyEzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_in_pinecone(docs):\n",
        "    texts = [doc.page_content for doc in docs]\n",
        "    embeddings = get_gemini_embeddings(texts)\n",
        "\n",
        "    # Store vectors in Pinecone\n",
        "    for i, (text, vector) in enumerate(zip(texts, embeddings)):\n",
        "        index.upsert([(str(i), vector.tolist(), {\"text\": text})])\n",
        "\n",
        "    print(\"Stored documents in Pinecone successfully!\")"
      ],
      "metadata": {
        "id": "m1kcOiOQZHpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/Gen AI Engineer - Machine Learning Engineer Assignment.pdf\"\n",
        "docs = load_and_split_pdf(pdf_path)\n",
        "store_in_pinecone(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1rZuqmTZeQj",
        "outputId": "1d2d6998-350a-43cf-c769-5fc63aab4079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stored documents in Pinecone successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Build RAG Pipeline**"
      ],
      "metadata": {
        "id": "yPb3xM99y-Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
        "\n",
        "def get_gemini_embedding(query):\n",
        "    \"\"\"Generate embeddings for the user query.\"\"\"\n",
        "    model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", task_type=\"retrieval_query\")\n",
        "    embedding = model.embed_query(query)\n",
        "    return np.array(embedding)\n",
        "\n",
        "def retrieve_relevant_docs(query, top_k=3):\n",
        "    \"\"\"Retrieve the most relevant documents from Pinecone.\"\"\"\n",
        "    query_vector = get_gemini_embedding(query).tolist()\n",
        "    results = index.query(vector=query_vector, top_k=top_k, include_metadata=True)\n",
        "\n",
        "    # Extract relevant texts\n",
        "    retrieved_texts = [match[\"metadata\"][\"text\"] for match in results[\"matches\"]]\n",
        "\n",
        "    return retrieved_texts\n",
        "\n",
        "def generate_rag_response(query):\n",
        "    \"\"\"Use RAG to fetch relevant documents and generate an answer.\"\"\"\n",
        "    retrieved_docs = retrieve_relevant_docs(query)\n",
        "    docs = \"\\n\\n\".join(retrieved_docs)\n",
        "\n",
        "    # Generate response using Gemini LLM\n",
        "    instructions = f\"\"\"You are a helpful assistant who is good at analyzing source information and answering questions.\n",
        "    Use the following source documents to answer the user's questions.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "    Documents:\n",
        "    {docs}\"\"\"\n",
        "\n",
        "    response = llm_model.invoke([\n",
        "            {\"role\": \"system\", \"content\": instructions},\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ],\n",
        "    )\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "ZGWTQ7ehhdDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"which type of dataset required to build this question answering model?\"\n",
        "response = generate_rag_response(query)\n",
        "print(\"Generated Answer:\\n\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbHUQKatiqX2",
        "outputId": "9ca27d3f-6643-434b-c266-ba6c2deef0a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Answer:\n",
            " A Profit & Loss (P&L) table extracted from PDF documents is needed.  The model also uses a vector database to store embeddings of financial terms and insights.  This allows for efficient retrieval of relevant information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_from_pinecone(state: PipelineState):\n",
        "    query = state[\"user_query\"]\n",
        "\n",
        "    # Load Pinecone as the retriever\n",
        "    responce = generate_rag_response(query)\n",
        "\n",
        "    return {\"response\": response}"
      ],
      "metadata": {
        "id": "sYf_XG9YYCFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **General Response**"
      ],
      "metadata": {
        "id": "JJTK5nYZzXag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def general_response(state: PipelineState):\n",
        "    query = state[\"user_query\"]\n",
        "\n",
        "    instructions = f\"\"\"\n",
        "    You are a helpful assistant, as a professional expert,\n",
        "    provide a concise and informative answer to the following question,\n",
        "    ensuring accuracy and addressing all relevant aspects, even if the\n",
        "    question appears broad or open-ended. Answer the following Question concisely.\n",
        "    \"\"\"\n",
        "\n",
        "    llm_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
        "\n",
        "    response = llm_model.invoke([\n",
        "            {\"role\": \"system\", \"content\": instructions},\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ],\n",
        "    )\n",
        "    return {\"response\": response.content}"
      ],
      "metadata": {
        "id": "qfpv34hYh-SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create LangGraph workflow\n",
        "workflow = StateGraph(PipelineState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"decision\", decision_node)\n",
        "workflow.add_node(\"weather\", fetch_weather)\n",
        "workflow.add_node(\"rag\", retrieve_from_pinecone)\n",
        "workflow.add_node(\"general\", general_response)\n",
        "\n",
        "# Define edges (execution flow)\n",
        "workflow.set_entry_point(\"decision\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"decision\",\n",
        "    lambda state: state[\"action\"],\n",
        "    {\n",
        "        \"weather\": \"weather\",\n",
        "        \"rag\": \"rag\",\n",
        "        \"general\": \"general\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# Finalize the graph\n",
        "graph = workflow.compile()"
      ],
      "metadata": {
        "id": "v_Tf8qaNiKl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "-4I7-iNVVmhf",
        "outputId": "96971327-642c-49a8-8555-dd4463cc6222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x783c80a92b90>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test LangGraph Performance**"
      ],
      "metadata": {
        "id": "g3vOXaINzlJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"What is the weather in Durgapur?\"\n",
        "\n",
        "state = {\"user_query\": user_input, \"action\": None, \"response\": None}\n",
        "output = graph.invoke(state)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "IHviBJmNiakR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec23fd01-3f36-47f1-921a-15a01bb454bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'user_query': 'What is the weather in Durgapur?', 'action': 'weather', 'response': 'Current weather in Durgapur: few clouds, Temperature: 28.65Â°C.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"which type of dataset required to build this question answering model in this pdf?\"\n",
        "\n",
        "state = {\"user_query\": user_input, \"action\": None, \"response\": None}\n",
        "output = graph.invoke(state)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFw0d31cntw5",
        "outputId": "b74dcc04-1b33-4eb8-8045-3b63ef98eb2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'user_query': 'which type of dataset required to build this question answering model in this pdf?', 'action': 'rag', 'response': 'A Profit & Loss (P&L) table extracted from PDF documents is needed.  The model also uses a vector database to store embeddings of financial terms and insights.  This allows for efficient retrieval of relevant information.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"What is Machine Learning\"\n",
        "\n",
        "state = {\"user_query\": user_input, \"action\": None, \"response\": None}\n",
        "output = graph.invoke(state)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ9k-Bz4v-Z4",
        "outputId": "44885600-2051-4f7c-fd9f-d0eef3fb0dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'user_query': 'What is Machine Learning', 'action': 'general', 'response': 'Machine learning (ML) is a subfield of artificial intelligence (AI) that focuses on enabling computer systems to learn from data without explicit programming.  Instead of relying on pre-programmed rules, ML algorithms identify patterns, make predictions, and improve their performance over time based on the data they are exposed to.  This learning process can be supervised (using labeled data), unsupervised (using unlabeled data), or reinforcement learning (using rewards and penalties).  ML powers numerous applications, including image recognition, natural language processing, and recommendation systems.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluate Q & A model**"
      ],
      "metadata": {
        "id": "qCdM4nC3zvGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LangChain LLM\n",
        "llm_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=1)\n",
        "\n",
        "# Define the RAG bot function\n",
        "@traceable()\n",
        "def rag_bot(question: str) -> dict:\n",
        "    docs = retrieve_relevant_docs(question)\n",
        "    docs_string = \"\\n\\n\".join(docs)\n",
        "\n",
        "    instructions = f\"\"\"You are a helpful assistant who is good at analyzing source information and answering questions.\n",
        "    Use the following source documents to answer the user's questions.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "    Documents:\n",
        "    {docs_string}\"\"\"\n",
        "\n",
        "    ai_msg = llm_model.invoke([\n",
        "            {\"role\": \"system\", \"content\": instructions},\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    return {\"answer\": ai_msg.content, \"documents\": docs}\n",
        "\n",
        "# Initialize LangSmith client\n",
        "client = Client()\n",
        "\n",
        "# Create dataset and examples\n",
        "dataset_name = \"Q&A Model for Evaluation \"\n",
        "dataset = client.create_dataset(dataset_name=dataset_name)\n",
        "\n",
        "examples = [\n",
        "    (\"What the main goal in this PDF assignment?\",\n",
        "     \"The main goal of this PDF assignment is to build a question-answering (QA) bot that can accurately answer questions about financial data extracted from P&L tables within PDF documents.  This involves two parts:  first, developing a Retrieval-Augmented Generation (RAG) model to efficiently retrieve relevant financial information; and second, creating an interactive user interface allowing users to upload PDFs, ask questions, and receive answers alongside supporting data from the document..\"),\n",
        "    (\"What platform use to build this question answering model in this PDF assignment?\",\n",
        "     \"The provided text doesn't specify the platform used to build the question-answering model.  It mentions using a vector database like Pinecone for storing and retrieving embeddings, and it describes the functionality of a RAG model, but it doesn't state the specific platform (e.g., Python with specific libraries, a cloud-based platform like AWS or Google Cloud, etc.) used for its development.\"),\n",
        "    (\"which type of dataset required to build this question answering model?\",\n",
        "     \"A Profit & Loss (P&L) table extracted from PDF documents is the required dataset.  The model uses this data to answer financial questions.  The P&L table should contain information on income, expenses, and profit margins.\"),\n",
        "]\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\": q} for q, _ in examples],\n",
        "    outputs=[{\"answer\": a} for _, a in examples],\n",
        "    dataset_id=dataset.id,\n",
        ")\n",
        "\n",
        "### Define Evaluators\n",
        "\n",
        "# Correctness Evaluation\n",
        "class CorrectnessGrade(BaseModel):\n",
        "    explanation: str = Field(description = \"Explain your reasoning for the score\")\n",
        "    correct: bool = Field(description = \"True if the answer is correct, False otherwise.\")\n",
        "\n",
        "correctness_instructions = \"\"\"You are a teacher grading a quiz.\n",
        "Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer.\n",
        "A correctness value of True means the answer is correct. False means it is not. Explain your reasoning.\"\"\"\n",
        "\n",
        "llm_grader = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0).with_structured_output(CorrectnessGrade)\n",
        "\n",
        "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
        "    answers = f\"\"\"QUESTION: {inputs['question']}\n",
        "    GROUND TRUTH ANSWER: {reference_outputs['answer']}\n",
        "    STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
        "\n",
        "    grade = llm_grader.invoke([{\"role\": \"system\", \"content\": correctness_instructions}, {\"role\": \"user\", \"content\": answers}])\n",
        "    return grade.correct\n",
        "\n",
        "# Relevance Evaluation\n",
        "class RelevanceGrade(BaseModel):\n",
        "    explanation: str = Field(description = \"Explain your reasoning for the score\")\n",
        "    relevant: bool = Field(description = \"True if the answer is relevant to the question\")\n",
        "\n",
        "relevance_instructions = \"\"\"You are a teacher grading a quiz.\n",
        "Ensure the student's answer is concise and relevant to the question.\n",
        "A relevance value of True means the answer meets these criteria; False means it does not.\"\"\"\n",
        "\n",
        "relevance_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0).with_structured_output(RelevanceGrade)\n",
        "\n",
        "def relevance(inputs: dict, outputs: dict) -> bool:\n",
        "    answer = f\"\"\"QUESTION: {inputs['question']}\n",
        "STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
        "\n",
        "    grade = relevance_llm.invoke([{\"role\": \"system\", \"content\": relevance_instructions}, {\"role\": \"user\", \"content\": answer}])\n",
        "    return grade.relevant\n",
        "\n",
        "# Groundedness Evaluation\n",
        "class GroundedGrade(BaseModel):\n",
        "    explanation: str = Field(description = \"Explain your reasoning for the score\")\n",
        "    grounded: bool = Field(description = \"True if the answer is based on the provided documents\")\n",
        "\n",
        "grounded_instructions = \"\"\"You are a teacher grading a quiz.\n",
        "Ensure the student's answer is grounded in the provided documents and does not include hallucinated information.\n",
        "A grounded value of True means the answer is well-supported; False means it is not.\"\"\"\n",
        "\n",
        "grounded_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0).with_structured_output(GroundedGrade)\n",
        "\n",
        "def groundedness(inputs: dict, outputs: dict) -> bool:\n",
        "    doc_string = \"\\n\".join(doc for doc in outputs[\"documents\"])\n",
        "    answer = f\"\"\"FACTS: {doc_string}\n",
        "STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
        "\n",
        "    grade = grounded_llm.invoke([{\"role\": \"system\", \"content\": grounded_instructions}, {\"role\": \"user\", \"content\": answer}])\n",
        "    return grade.grounded\n",
        "\n",
        "# Define the evaluation target\n",
        "def target(inputs: dict) -> dict:\n",
        "    return rag_bot(inputs[\"question\"])\n",
        "\n",
        "# Run the evaluation\n",
        "experiment_results = client.evaluate(\n",
        "    target,\n",
        "    data=dataset_name,\n",
        "    evaluators=[correctness, relevance, groundedness],\n",
        "    experiment_prefix=\"rag-doc-relevance\",\n",
        "    metadata={\"version\": \"1.0\"}\n",
        ")"
      ],
      "metadata": {
        "id": "_CCYFM1vtj8_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "b5302176cbb849069da50e46483ac093",
            "88a2b72bf11b4ef5af57ff3e65dcd578",
            "6295b201c83d499c925ca4140007af28",
            "08cbe880c7b542fcb1febace0b58375e",
            "36aa65391552403eae5a081b5cfd5819",
            "edb187f5218941d1869157a42624ca4b",
            "7d00655246254c8f9d205ac7d140de74",
            "c9506c24c79e42eb9c30af33899ec73a",
            "22660f1a37fc4f6c8aba3762002db15a",
            "72665e9082d5439fa853e6aeb44d0f91",
            "ee3f1d82392c4ad8ba0506230481fcbe"
          ]
        },
        "outputId": "f1477af7-cff7-4140-dbcf-f430eed16971"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'rag-doc-relevance-0d6084b8' at:\n",
            "https://smith.langchain.com/o/1a930940-6f2b-4a80-a11b-277499a43884/datasets/e79890b3-b7d9-4b01-b4f4-d3646bb3d00b/compare?selectedSessions=7b29503f-fd16-44b5-9aae-c6deb4c5733d\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5302176cbb849069da50e46483ac093"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test Cases**"
      ],
      "metadata": {
        "id": "5H_iq_UA1OVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytest"
      ],
      "metadata": {
        "id": "aivr6-Ok2IoZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a80186-d843-414d-f42f-76a59806de6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (8.3.4)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest) (1.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"pipeline\")\n",
        "os.makedirs(\"test\")"
      ],
      "metadata": {
        "id": "1Z8VuoDurXEh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/pipeline/pipeline.py\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import numpy as np\n",
        "import langchain\n",
        "import langgraph\n",
        "from dotenv import load_dotenv\n",
        "from langgraph.graph import Graph\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "from langsmith import traceable, Client\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from typing import TypedDict, Optional\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Import the required classes directly\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.schema import Document\n",
        "\n",
        "index_name = 'qa-bot'\n",
        "PINE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "from pinecone.grpc import PineconeGRPC as Pinecone\n",
        "from pinecone import ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=PINE_API_KEY)\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "class PipelineState(TypedDict):\n",
        "    user_query: str\n",
        "    action: Optional[str]\n",
        "    response: Optional[str]\n",
        "\n",
        "def decision_node(state: PipelineState):\n",
        "    query = state[\"user_query\"]\n",
        "\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
        "\n",
        "    classification_prompt = f\"\"\"\n",
        "    Classify this query as either 'weather', 'rag', or 'general'.\n",
        "    - If it is asking about weather conditions, classify as 'weather'.\n",
        "    - If it is asking about information from a provided document, classify as 'rag'.\n",
        "    - If the query does not fit into these categories, classify as 'general'.\n",
        "\n",
        "    Query: {query}\n",
        "    \"\"\"\n",
        "\n",
        "    action = llm.invoke(classification_prompt)\n",
        "\n",
        "    # Ensure action is valid\n",
        "    action_str = action.content.lower()\n",
        "    if action_str not in [\"weather\", \"rag\", \"general\"]:\n",
        "        action_str = \"general\"\n",
        "    else:\n",
        "        action_str = action.content.lower()\n",
        "\n",
        "    return {\"action\": action_str}\n",
        "\n",
        "def fetch_weather(state: PipelineState):\n",
        "    user_query = state[\"user_query\"]\n",
        "\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0)\n",
        "    # Prompt template for extracting city name\n",
        "    city_extraction_prompt = PromptTemplate.from_template(\n",
        "        \"Extract the city name from this query: {query}. If no city is found, return 'None'.\"\n",
        "    )\n",
        "    # Ask Gemini LLM to extract the city name\n",
        "    prompt = city_extraction_prompt.format(query=user_query)\n",
        "    city = llm.invoke(prompt)\n",
        "\n",
        "    if city.content.lower() == \"none\":\n",
        "        return {\"response\": \"I couldn't determine the city name. Please specify a valid location.\"}\n",
        "\n",
        "    api_key = os.getenv(\"OPENWEATHER_API_KEY\")\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city.content}&appid={api_key}&units=metric\"\n",
        "\n",
        "    response = requests.get(url).json()\n",
        "\n",
        "    if response.get(\"main\"):\n",
        "        weather_info = f\"Current weather in {city.content}: {response['weather'][0]['description']}, Temperature: {response['main']['temp']}Â°C.\"\n",
        "    else:\n",
        "        weather_info = \"Weather data not available. Please check the city name.\"\n",
        "\n",
        "    return {\"response\": weather_info}\n",
        "\n",
        "\n",
        "def load_and_split_pdf(pdf_path):\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into smaller chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    return docs\n",
        "\n",
        "def get_gemini_embeddings(texts, batch_size = 10):\n",
        "    model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", task_type=\"retrieval_document\")\n",
        "    embeddings = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i : i + batch_size]\n",
        "\n",
        "        try:\n",
        "            response = model.embed_documents(batch)\n",
        "            batch_embeddings = [item for item in response]\n",
        "            embeddings.extend(batch_embeddings)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {i}: {e}\")\n",
        "            time.sleep(5)\n",
        "    return np.array(embeddings)\n",
        "\n",
        "\n",
        "def store_in_pinecone(docs):\n",
        "    texts = [doc.page_content for doc in docs]\n",
        "    embeddings = get_gemini_embeddings(texts)\n",
        "\n",
        "    # Store vectors in Pinecone\n",
        "    for i, (text, vector) in enumerate(zip(texts, embeddings)):\n",
        "        index.upsert([(str(i), vector.tolist(), {\"text\": text})])\n",
        "    print(\"Stored documents in Pinecone successfully!\")\n",
        "\n",
        "def get_gemini_embedding(query):\n",
        "    \"\"\"Generate embeddings for the user query.\"\"\"\n",
        "    model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", task_type=\"retrieval_query\")\n",
        "    embedding = model.embed_query(query)\n",
        "    return np.array(embedding)\n",
        "\n",
        "def retrieve_relevant_docs(query, top_k=3):\n",
        "    \"\"\"Retrieve the most relevant documents from Pinecone.\"\"\"\n",
        "    query_vector = get_gemini_embedding(query).tolist()\n",
        "    results = index.query(vector=query_vector, top_k=top_k, include_metadata=True)\n",
        "\n",
        "    # Extract relevant texts\n",
        "    retrieved_texts = [match[\"metadata\"][\"text\"] for match in results[\"matches\"]]\n",
        "\n",
        "    return retrieved_texts\n",
        "\n",
        "\n",
        "def generate_rag_response(query):\n",
        "    \"\"\"Use RAG to fetch relevant documents and generate an answer.\"\"\"\n",
        "    retrieved_docs = retrieve_relevant_docs(query)\n",
        "    docs = \"\\n\\n\".join(retrieved_docs)\n",
        "\n",
        "    # Generate response using Gemini LLM\n",
        "    instructions = f\"\"\"You are a helpful assistant who is good at analyzing source information and answering questions.\n",
        "    Use the following source documents to answer the user's questions.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "    Documents:\n",
        "    {docs}\"\"\"\n",
        "\n",
        "    llm_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
        "    response = llm_model.invoke([\n",
        "            {\"role\": \"system\", \"content\": instructions},\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ],\n",
        "    )\n",
        "    return response.content\n",
        "\n",
        "def retrieve_from_pinecone(state: PipelineState):\n",
        "    query = state[\"user_query\"]\n",
        "\n",
        "    # Load Pinecone as the retriever\n",
        "    response = generate_rag_response(query)\n",
        "\n",
        "    return {\"response\": response}\n",
        "\n",
        "def general_response(state: PipelineState):\n",
        "    query = state[\"user_query\"]\n",
        "\n",
        "    instructions = f\"\"\"\n",
        "    You are a helpful assistant, as a professional expert,\n",
        "    provide a concise and informative answer to the following question,\n",
        "    ensuring accuracy and addressing all relevant aspects, even if the\n",
        "    question appears broad or open-ended. Answer the following Question concisely.\n",
        "    \"\"\"\n",
        "\n",
        "    llm_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
        "\n",
        "    response = llm_model.invoke([\n",
        "            {\"role\": \"system\", \"content\": instructions},\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ],\n",
        "    )\n",
        "    return {\"response\": response.content}\n",
        "\n",
        "\n",
        "# Create LangGraph workflow\n",
        "workflow = StateGraph(PipelineState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"decision\", decision_node)\n",
        "workflow.add_node(\"weather\", fetch_weather)\n",
        "workflow.add_node(\"rag\", retrieve_from_pinecone)\n",
        "workflow.add_node(\"general\", general_response)\n",
        "\n",
        "# Define edges (execution flow)\n",
        "workflow.set_entry_point(\"decision\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"decision\",\n",
        "    lambda state: state[\"action\"],\n",
        "    {\n",
        "        \"weather\": \"weather\",\n",
        "        \"rag\": \"rag\",\n",
        "        \"general\": \"general\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# Finalize the graph\n",
        "graph = workflow.compile()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF88NOX6HFbB",
        "outputId": "0996eff4-873c-4339-9d91-914f75350f42"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/pipeline/pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test/test_pipeline.py\n",
        "import pytest\n",
        "from unittest.mock import patch, MagicMock\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the project root to the Python path\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
        "\n",
        "# Import the pipeline module\n",
        "from pipeline.pipeline import (\n",
        "    decision_node, fetch_weather, retrieve_from_pinecone,\n",
        "    general_response, get_gemini_embedding, generate_rag_response, retrieve_relevant_docs\n",
        ")\n",
        "\n",
        "### TEST CASES ###\n",
        "\n",
        "def test_decision_node():\n",
        "    \"\"\"Test classification of user queries into correct categories.\"\"\"\n",
        "    test_cases = [\n",
        "        (\"What's the weather in New York?\", \"weather\"),\n",
        "        (\"Tell me about quantum physics.\", \"general\"),\n",
        "        (\"Summarize this document.\", \"rag\")\n",
        "    ]\n",
        "\n",
        "    for query, expected_action in test_cases:\n",
        "        state = {\"user_query\": query}\n",
        "        result = decision_node(state)\n",
        "        assert result[\"action\"] == expected_action, f\"Failed for query: {query}\"\n",
        "\n",
        "@patch(\"pipeline.pipeline.requests.get\")\n",
        "def test_fetch_weather(mock_get):\n",
        "    \"\"\"Test weather API response handling.\"\"\"\n",
        "    mock_get.return_value.json.return_value = {\n",
        "        \"main\": {\"temp\": 22.5},\n",
        "        \"weather\": [{\"description\": \"clear sky\"}]\n",
        "    }\n",
        "\n",
        "    state = {\"user_query\": \"What is the weather in London?\"}\n",
        "    result = fetch_weather(state)\n",
        "\n",
        "    assert \"London\" in result[\"response\"]\n",
        "    assert \"clear sky\" in result[\"response\"]\n",
        "    assert \"22.5Â°C\" in result[\"response\"]\n",
        "\n",
        "@patch(\"pipeline.pipeline.GoogleGenerativeAIEmbeddings.embed_query\")\n",
        "def test_get_gemini_embedding(mock_embed):\n",
        "    \"\"\"Ensure query embeddings return correct shape.\"\"\"\n",
        "    mock_embed.return_value = [0.1] * 768  # Mock embedding vector\n",
        "    embedding = get_gemini_embedding(\"What is AI?\")\n",
        "    assert len(embedding) == 768, \"Embedding shape mismatch!\"\n",
        "\n",
        "@patch(\"pipeline.pipeline.ChatGoogleGenerativeAI.invoke\")\n",
        "def test_general_response(mock_invoke):\n",
        "    \"\"\"Test general response generation using LLM.\"\"\"\n",
        "    mock_invoke.return_value = MagicMock(content=\"Artificial Intelligence is a branch of science.\")\n",
        "\n",
        "    state = {\"user_query\": \"What is AI?\"}\n",
        "    result = general_response(state)\n",
        "\n",
        "    assert \"Artificial Intelligence\" in result[\"response\"]\n",
        "\n",
        "@patch(\"pipeline.pipeline.ChatGoogleGenerativeAI.invoke\")\n",
        "def test_generate_rag_response(mock_invoke):\n",
        "    \"\"\"Test RAG-based response generation.\"\"\"\n",
        "    mock_invoke.return_value = MagicMock(content=\"AI is a field of study focusing on machine intelligence.\")\n",
        "\n",
        "    response = generate_rag_response(\"Tell me about AI.\")\n",
        "\n",
        "    assert \"AI is a field of study\" in response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1jyXB0GHWuz",
        "outputId": "e9993220-6fa6-4aa9-aa59-56101df37034"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test/test_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest test/test_pipeline.py -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ_y1AErH0ws",
        "outputId": "60409741-4992-44ab-896a-10a887ff9d3b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytest_asyncio/plugin.py:207: PytestDeprecationWarning: The configuration option \"asyncio_default_fixture_loop_scope\" is unset.\n",
            "The event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n",
            "\n",
            "  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\n",
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.4, pluggy-1.5.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: socket-0.7.0, syrupy-4.8.2, asyncio-0.25.3, anyio-3.7.1, typeguard-4.4.2, langsmith-0.3.11\n",
            "asyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None\n",
            "collected 5 items                                                                                  \u001b[0m\n",
            "\n",
            "test/test_pipeline.py::test_decision_node \u001b[32mPASSED\u001b[0m\u001b[32m                                             [ 20%]\u001b[0m\n",
            "test/test_pipeline.py::test_fetch_weather \u001b[32mPASSED\u001b[0m\u001b[32m                                             [ 40%]\u001b[0m\n",
            "test/test_pipeline.py::test_get_gemini_embedding \u001b[32mPASSED\u001b[0m\u001b[32m                                      [ 60%]\u001b[0m\n",
            "test/test_pipeline.py::test_general_response \u001b[32mPASSED\u001b[0m\u001b[32m                                          [ 80%]\u001b[0m\n",
            "test/test_pipeline.py::test_generate_rag_response \u001b[32mPASSED\u001b[0m\u001b[32m                                     [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 4.10s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok"
      ],
      "metadata": {
        "id": "YFQTc6l05bGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the project root to the Python path\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
        "\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Initialize LangGraph workflow\n",
        "from pipeline.pipeline import graph  # Import the LangGraph pipeline\n",
        "\n",
        "st.set_page_config(page_title=\"Chatbot with RAG & Weather\", page_icon=\"ð¤\")\n",
        "\n",
        "st.title(\"ð¤ Chatbot with RAG & Weather\")\n",
        "\n",
        "# Conversation memory\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display previous messages\n",
        "for msg in st.session_state.messages:\n",
        "    with st.chat_message(msg[\"role\"]):\n",
        "        st.write(msg[\"content\"])\n",
        "\n",
        "# Get user input\n",
        "user_query = st.chat_input(\"Ask me anything...\")\n",
        "\n",
        "if user_query:\n",
        "    # Save user query\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_query})\n",
        "\n",
        "    # Run the LangGraph pipeline\n",
        "    response = graph.invoke({\"user_query\": user_query})[\"response\"]\n",
        "\n",
        "    # Save bot response\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "    # Display response\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.write(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFKqfZnA5fYH",
        "outputId": "1d18d265-7a3c-4506-fb3f-e98837b9a69a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "!ngrok config add-authtoken <ngrok api keys>\n",
        "# Kill previous tunnels\n",
        "!pkill streamlit\n",
        "!pkill ngrok\n",
        "\n",
        "# Run Streamlit app in the background\n",
        "os.system(\"streamlit run app.py &\")\n",
        "\n",
        "# Expose the app via ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Streamlit App URL:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0QFLX6H5zgR",
        "outputId": "d02466f7-8810-41f8-ad57-45803b25fffb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Streamlit App URL: NgrokTunnel: \"https://09b1-34-148-149-7.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWbwFi/Mlu5Ua+Rnme3air",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b5302176cbb849069da50e46483ac093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88a2b72bf11b4ef5af57ff3e65dcd578",
              "IPY_MODEL_6295b201c83d499c925ca4140007af28",
              "IPY_MODEL_08cbe880c7b542fcb1febace0b58375e"
            ],
            "layout": "IPY_MODEL_36aa65391552403eae5a081b5cfd5819"
          }
        },
        "88a2b72bf11b4ef5af57ff3e65dcd578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edb187f5218941d1869157a42624ca4b",
            "placeholder": "â",
            "style": "IPY_MODEL_7d00655246254c8f9d205ac7d140de74",
            "value": ""
          }
        },
        "6295b201c83d499c925ca4140007af28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9506c24c79e42eb9c30af33899ec73a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22660f1a37fc4f6c8aba3762002db15a",
            "value": 1
          }
        },
        "08cbe880c7b542fcb1febace0b58375e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72665e9082d5439fa853e6aeb44d0f91",
            "placeholder": "â",
            "style": "IPY_MODEL_ee3f1d82392c4ad8ba0506230481fcbe",
            "value": "â3/?â[00:09&lt;00:00,ââ3.15s/it]"
          }
        },
        "36aa65391552403eae5a081b5cfd5819": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edb187f5218941d1869157a42624ca4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d00655246254c8f9d205ac7d140de74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9506c24c79e42eb9c30af33899ec73a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "22660f1a37fc4f6c8aba3762002db15a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72665e9082d5439fa853e6aeb44d0f91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee3f1d82392c4ad8ba0506230481fcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}